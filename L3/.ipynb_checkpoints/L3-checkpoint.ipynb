{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================================\n",
    "#  3A - L1 Camera and Images\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an image?\n",
    "\n",
    "- In previous lesson: a function- a 2D pattern of intensity values\n",
    "- Here: 1 2D projection of 3D points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First known Photograph - Heliograph\n",
    "\n",
    "The first photograph was taken by Joseph Nicephore Niepce in 1826. The below image is a reproduction made in 1952\n",
    "\n",
    "<figure>\n",
    "  <img src=\"first_photo.jpg\" /> \n",
    "    <center><figcaption>Fig.1 The reproduction of the first photograph <a href=\"https://en.wikipedia.org/wiki/History_of_photography\">Source</a></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging System\n",
    "\n",
    "### What is a camera?\n",
    "It's some device that allows the <u>**projection**</u> of light from three dimensions, to some medium (film, sensor,..etc) that will record the light pattern.\n",
    "\n",
    "<br />\n",
    "\n",
    "When you project a scene to the camera, you lose 3D information, and you have to find out what went out in the lost dimesnion. For instance, the image below were taken by a camera from front and right side. The projection from the right side shows the realistic depection of the globe painted on the side walk. \n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"projection_ex.png\" />\n",
    "  <center><figcaption>Fig.2(a) Example of projection. Notice the strange guy at the top of the globe</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"projection_ex2.png\" />\n",
    "  <center><figcaption>Fig.2(b) The real view of the projection as appears from right side</figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Formation\n",
    "\n",
    "As shown in figure 3(a), a light is reflected from a point to a film will create an image. However, a single point will reflect lights all over the film. In order to solve this problem, we place a barrier, with a small holes **(aperture)** in it that each will allow single light through it to the film as in figure 3(b)\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"image_formation_1.png\" />\n",
    "  <center><figcaption>Fig.3(a) Light comes from a point poroject all over the film  </figcaption></center>\n",
    "</figure>\n",
    "<figure>\n",
    "  <img src=\"image_formation_2.png\" />\n",
    "  <center><figcaption>Fig.3(b) A barrier with small holls (aperture) is placed between the scene and the film to allow only single light angle to the film   </figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperture\n",
    "\n",
    "<font color=\"blue\">How does aperture affect the image that we see? How does the size affect the image?</font>\n",
    "\n",
    "The bluriness of the image is directly related to the size of the arpeture. As the arpeture size decreases, the bluriness decrease and the crisper images are produced. Aperture sizes are in mm (e.g. 2mm, 1mm, 0.6mm, and 0.35mm)\n",
    "\n",
    "<font color=\"blue\">Why not making the size very very small?</font>\n",
    "\n",
    "Because of diffraction effect due to the wave nature of light.\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"starburst_night.jpg\" />\n",
    "    <center><figcaption>Fig.4 effect of aperture size on the image <a href=\"https://www.slrlounge.com/diffraction-aperture-and-starburst-effects/\">source</a> </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenses\n",
    "\n",
    "We don't really use pinhole cameras anymore. Instead, we use lenses. Lenses are desinged to project all lights comming from a point at a particular distance away to the same point on the film. However, as in Figure 5, the challange is that lights comming from different distance will be projected at slightly different points on the film (circle of confusion)\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"lenses_1.png\" />\n",
    "  <center><figcaption>Fig.5(a): Shows lenses' concept and the circle of confusion </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<font color=\"blue\">Lenses Components</font>\n",
    "- Optical axis: light passes straight through unbent\n",
    "- Paralell lights are all bent to the same point\n",
    "- Focal point: a point where parallel lights are bent to\n",
    "- Aperture: to reduce the amount of spread of things, by restricting the set of rays that are coming in.\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"lenses_2.png\" />\n",
    "  <center><figcaption>Fig.5(b): Shows lenses' components</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thin Lenses\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<figure>\n",
    "  <img src=\"thin_lenses.png\" />\n",
    "  <center><figcaption>Fig.6(a) depicts thin lenses</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"thin_lenses_2.png\" />\n",
    "  <center><figcaption>Fig.6(b) film lens diagram</figcaption></center>\n",
    "</figure>\n",
    "<figure>\n",
    "  <img src=\"thin_lenses_3.png\" />\n",
    "  <center><figcaption>Fig.6(c) film lens diagram</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<font color=\"blue\">Can we predict the relationship between, say $d_0$ and $d_i$ given lens with a given focal point where $d_0$ is the distance of the object, and $d_i$ is the distance from the lens to the image (Figure 6(a))?</font><br />\n",
    "\n",
    "Using similar triangles in Figure 6(b), we can reason that given the point $P$ at $y$ and projected at point $P'$ at $-y$ \n",
    "\n",
    "$$\\frac{-y'}{y} = \\frac{||z'||}{||z||}$$\n",
    "\n",
    "Also, using similar triangles in Figure 6(c), we can reason that\n",
    "\n",
    "$$\\frac{-y'}{y} = \\frac{||z'|| - f}{f}$$\n",
    "\n",
    "\n",
    "$$\\implies \\frac{1}{||z||} = \\frac{1}{f} - \\frac{1}{||z'||}$$\n",
    "\n",
    "$$\\implies \\frac{1}{f} =  \\frac{1}{||z||} + \\frac{1}{||z'||}$$ \n",
    "<br/>\n",
    "<center>(Thin Lense Equation)</center>\n",
    "\n",
    "\n",
    "$y$: height of a point from the optical axis <br/>\n",
    "$y'$: height of the projection of $P$ from the optical axis <br/>\n",
    "$z$: distance to the world <br/>\n",
    "$z'$: distance from lense to the image plane <br/>\n",
    "\n",
    "<font color=\"green\">So, any point that satisfy the thin lense Equation will be in focus. Hence, by moving the lense back and forward, we change where in the world things are in focus</font> <br />\n",
    "\n",
    "\n",
    "Check out: http://www.phy.ntnu.edu.tw/ntnujava/index.php?topic=48\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color=\"blue\">**Quize**</font>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"thin_lenses_quiz.png\" />\n",
    "  <center><figcaption>Fig.6(d): Thin lense quiz</figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Z1 == 1000.000000, Z' == 52.631579\n",
      "When Z1 == 2000.000000, Z' == 51.282051\n"
     ]
    }
   ],
   "source": [
    "f = 50.0\n",
    "z = [1000.0,2000.0] # in mm\n",
    "for zi in z:\n",
    "    zp = 1/(1/f - 1/zi)\n",
    "    print(\"When Z1 == %f, Z' == %f\" % (zi,zp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Focus\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"varying_focus.gif\" />\n",
    "    <center><figcaption>Fig.7: The effect of varying focus <a href=\"https://www269.lunapic.com/editor/?action=focus-anim\"> Source</a></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth of field\n",
    "\n",
    "It is concerned with how much of focus change occurs for a certain point as the lense move in and out. The DoF is controlled by the aperture. With wide aperture, the rays diverge quite a bit, and hence moving the plane in a little bit cause the rays to spread out alot. On the other hand, with smaller aperture, the spread is much less as the plane is moving in (Figure 8a,b). \n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"dof.png\" />\n",
    "    <center><figcaption>Fig.8 (a): depth of field <a href=\"http://en.wikipedia.org/wiki/Depth_of_field\">Source</a></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"dof2.jpg\" />\n",
    "    <center><figcaption>Fig.8 (b): change of aperture <a href=\"http://rubbingpixels.com/depth-field-used-photography/\">Source</a></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field of View (zooming)\n",
    "\n",
    "How wide of a view do we have. There are two types of lens that can change their field of view:\n",
    "\n",
    "1. changing focal lenght while staying in constant focus\n",
    "2. changing focal length but must refocus (variable focal length lenses)\n",
    "\n",
    "When the focal length is high, the angular deflection moves the pixels drastically and thus any shake in the camera will shake the image greatly. So the more the focal length, the more we need to stabilize the camera. \n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fov2.jpg\" />\n",
    "  <center><figcaption>Fig.9 (a): change in field of view as we change focal length <a href=\"http://www.capturedbyone.com/cbo-blog/know-your-focal-length-and-lens-field-of-view\">Source</a></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fov.jpg\" />\n",
    "  <center><figcaption>Fig.9 (b): example change in field of view as we change focal length <a href=\"https://www.bhphotovideo.com/explora/photography/buying-guide/prime-lens-101\">Source</a></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "###  FOV depends on Focal Length\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"fov3.png\" />\n",
    "  <center><figcaption>Fig.9 (c): Calculating FOV </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "The field of view can be calculated by calculating the angle $\\phi$ depected in Figure 9(c). \n",
    "\n",
    "$$\\phi = tan^-1(\\frac{d/2}{f})$$\n",
    "\n",
    "$d$: the retina or sensor size\n",
    "\n",
    "<font color=\"green\">Larger Focul Length $\\implies$ Smaller FOV <br/></font>\n",
    "<font color=\"green\">Bigger the imaging surface $\\implies$ Bigger FOV</font>\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color=\"blue\">**Quize**</font>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"fov_quiz.png\" />\n",
    "  <center><figcaption>Fig.9(d): Field of view quiz</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{1}{f} =  \\frac{1}{||z||} + \\frac{1}{||z'||}$$\n",
    "$$\\implies \\frac{1}{f} =  \\frac{z' + z}{z'z}$$\n",
    "$$\\implies fz' + fz = zz'$$\n",
    "$$\\implies fz' = zz' - fz$$\n",
    "$$\\implies fz' = z(z' - f)$$\n",
    "$$\\implies z = \\frac{z'f}{z'-f} \\,\\,(1)$$  \n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\\frac{d/2}{z'} =  \\frac{w/2}{z}$$\n",
    "$$\\implies z =  \\frac{w/d}{z'} \\,\\,(2)$$ \n",
    "\n",
    "<br />\n",
    "\n",
    "<center>from 1 & 2</center>\n",
    "\n",
    "\n",
    "$$\\implies \\frac{z'f}{z'-f} =  \\frac{w/d}{z'}$$\n",
    "$$\\implies w(z'-f) =  df$$\n",
    "$$\\implies z' -f  =  \\frac{df}{w}$$\n",
    "$$\\implies z'  =  \\frac{df}{w} + f$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05 52.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "w = 0.7*1000 #to mm\n",
    "f = 50\n",
    "d = 35\n",
    "zp = d*f/w + f ## df/w + f\n",
    "z = zp*w/d ## z = z'w/d\n",
    "\n",
    "print(z/1000,zp) # to m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zooming and Moving are not the same\n",
    "\n",
    "In Figure 10(a), the image on the left image were taken with a large FOV, and small $f$ while the camera is close to the car. The image on the right, on the other hand, is taken with small FOV, and large $f$ while the camera is far from the car. You can notice that the shape is different. The left image has what is called *Perspctive Distortion*. \n",
    "\n",
    "Figure 10(b) gives another example of *Perspctive Distortion* as we minimize $f$ and move closer to the face\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"zoomingvsmoving.png\" />\n",
    "  <center><figcaption>Fig.10 (a): zooming vs moving </figcaption></center>\n",
    "</figure>\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"zoomingvsmoving2.png\" />\n",
    "  <center><figcaption>Fig.10 (b): zooming vs moving </figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dolly Zoom\n",
    "\n",
    "When the camera move closer and closer to an object while widening the lense at the same time, the object in the middle will have the same size, and to maintain its stationary point. However, this will create an illusionary scene where the stuff on the outside will grow. \n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"zvmi3.gif\" />\n",
    "  <center><figcaption>Fig.11: Dolly Zoom</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenses Are Not Perfect\n",
    "\n",
    "There are many problems that might arise from lenses, photographers, or the scene.\n",
    "\n",
    "### Geometric Distortion\n",
    "\n",
    "You can correct geometric distortion in post-imaging tools like photoshop if you know the focal length of your camera Figure 12(b)\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"gmd.png\" />\n",
    "  <center><figcaption>Fig.12 (a): Geometric Distortion</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"gmdc.png\" />\n",
    "  <center><figcaption>Fig.12 (b): Correction of Radial Distortion</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "### Chromatic Aberration\n",
    "\n",
    "This problem happens when rays of different wavelength focus in different planes. The problem is how to get all lights from a certain point to land on same point on the image. Notice the red line in the bigger picture in Figure 12(d)\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"chrabb.png\" />\n",
    "  <center><figcaption>Fig.12 (c): Chromatic Aberration</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"chrabb2.png\" />\n",
    "  <center><figcaption>Fig.12 (d): Chromatic Aberration Example</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "### Vignetting\n",
    "\n",
    "With some camera settings, not all rays that are hitting in the middle of the image are being caught at the corners. \n",
    "\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "<img src=\"vignetting2.png\" />\n",
    "  <center><figcaption>Fig.12 (e): Vignetting <a href=\"https://photographylife.com/what-is-vignetting\">Source</a></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "<img src=\"vignetting3.jpg\" />\n",
    "  <center><figcaption>Fig.12 (f): Example of Vignetting <a href=\"https://forums.bohemia.net/forums/topic/146786-vignetting/\">Source</a></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lense Systems\n",
    "\n",
    "Advanced cameras have multiple lenses. The camera in Figure 13 has 15 lenses that change according to the need.  \n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"lenses_system.png\" />\n",
    "  <center><figcaption>Fig.13: Example of advanced camera lenses</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================================\n",
    "#  3A - L2 Perspective Imaging\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Systems\n",
    "\n",
    "- Using the pinhole model as an approximation\n",
    "    - Put the optical center (Center of Projection) at the origin\n",
    "    - Standard (x,y) Coordinate System\n",
    "    - Put the image plane (Projection Plane) in front of the COP (for math convenient)\n",
    "    - The cameral looks down the negative z axis\n",
    "    \n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"coordinate_system.png\" />\n",
    "  <center><figcaption>Fig.14: depects the coordinate system for modeling projection</figcaption></center>\n",
    "</figure>\n",
    "    \n",
    "\n",
    "<br />\n",
    "\n",
    "### Projection Equations\n",
    "\n",
    "- Compute intersection with Perspective Projection of ray from (x,y,z) to COP\n",
    "- Derived using similar triangles\n",
    "\n",
    "$$(X,Y,Z) -> (-d\\frac{X}{Z},-d\\frac{Y}{Z},-d)$$\n",
    "\n",
    "\n",
    "So, we can get the projection by throwing out the last coordinate:\n",
    "\n",
    "$$(x', y') = (-d\\frac{X}{Z},-d\\frac{Y}{Z})$$\n",
    "\n",
    "<font color=\"green\">**When dividing by Z, distant objects are smaller</font>\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color=\"blue\">**Quize**</font>\n",
    "\n",
    "When objects are very far away, the real X and real Z can be huge. If I move the camera (the origin), those numbers hardly change. This explains:\n",
    "\n",
    "a) Why the moon follows you<br />\n",
    "b) Why the North Star is always North<br />\n",
    "c) Why you can tell time from the Sun regardless of where you are?<br />\n",
    "<font color=\"green\">d) All of the above</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Coordinates\n",
    "\n",
    "\n",
    "Is this a linear transformation?<br />\n",
    "No- division by the (not constant) Z is non-linear<br />\n",
    "\n",
    "<font color=\"green\">Trick: add one more coordinate:</font>\n",
    "\n",
    "Homogeneous image (2D) coordinates$\\,\\,\\,(x,y)\\,\\,\\, \\Rightarrow \\begin{bmatrix} x \\\\ y \\\\ 1\\end{bmatrix} $<br /><br />\n",
    "Homogeneous scene (3D) coordinates$\\,\\,\\,(x,y,z) \\Rightarrow \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix} $\n",
    "\n",
    "\n",
    "#### Converting from homogeneous coordinates: \n",
    "$$\\begin{bmatrix} x \\\\ y \\\\ w\\end{bmatrix} \\,\\,\\, \\Rightarrow \\,\\,\\,(x/w,y/w)$$\n",
    "<br />\n",
    "<br />\n",
    "$$\\begin{bmatrix} x \\\\ y \\\\ z \\\\ w\\end{bmatrix} \\,\\,\\, \\Rightarrow \\,\\,\\,(x/w,y/w,z/w)$$\n",
    "\n",
    "\n",
    "<font color=\"green\">**This makes homogenous coordinates invariant under scale</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Projection\n",
    "projetion is a matrix multiply using homogeneous coordinates:\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{f} & 0  \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ \\frac{z}{f}\\end{bmatrix}  \\,\\,\\, \\Rightarrow \\,\\,\\,(f\\frac{x}{z},f\\frac{y}{z})$$\n",
    "\n",
    "\n",
    "$$\\Rightarrow (u,v)$$\n",
    "\n",
    "$f$: is the focal length, the distance from the center of projection to the image plane\n",
    "\n",
    "<font color=\"blue\">How does scaling the projection matrix change the transformation?</font>\n",
    "\n",
    "$$\\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0  \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix} = \\begin{bmatrix} fx \\\\ fy \\\\ z \\end{bmatrix}  \\,\\,\\, \\Rightarrow \\,\\,\\,(f\\frac{x}{z},f\\frac{y}{f})$$\n",
    "\n",
    "<font color=\"green\">**So invariant under scale</font>\n",
    "\n",
    "<font color=\"blue\">**Quize**</font>\n",
    "\n",
    "Given Point p in 3-space[x y z] and focal length f, write a function that returns the location of the projected point on 2D image plane [u v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def project(p,f):\n",
    "    A = np.array([[1,0,0,0],[0,1,0,0],[0,0,1/f,0]])\n",
    "    p = np.append(p,1)\n",
    "    mul = np.matmul(A,p.T)\n",
    "    return (mul[0]/mul[2],mul[1]/mul[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200.0, 100.0)\n",
      "(100.0, 50.0)\n"
     ]
    }
   ],
   "source": [
    "p1 = np.array([200,100,50])\n",
    "p2 = np.array([200,100,100])\n",
    "f = 50\n",
    "print(project(p1,f))\n",
    "print(project(p2,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric properties of projection\n",
    "\n",
    "- Points go to points\n",
    "- Lines go to lines\n",
    "- Polygons go to polygons\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"proj_gpr.png\" />\n",
    "  <center><figcaption>Fig.15: properties of projection</figcaption></center>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Lines in the World Meet in the Image (Vanishing) point\n",
    "\n",
    "\n",
    "Line in 3-space\n",
    "\n",
    "$$x(t) = x_0 + at$$\n",
    "$$y(t) = y_0 + bt$$\n",
    "$$z(t) = z_0 + ct$$\n",
    "\n",
    "$x_0, y_0,z_0$:  is starting point <br/>\n",
    "$at,bt,zt$:  moving on vector <br/>\n",
    "\n",
    "\n",
    "#### Perspective projection of the line\n",
    "\n",
    "$$x'(t) = \\frac{fx}{z} =  \\frac{f(x_{0} + at)}{z_0 + ct} $$\n",
    "<br />\n",
    "$$y'(t) = \\frac{fy}{z} =  \\frac{f(y_{0} + bt)}{z_0 + ct}$$\n",
    "\n",
    "<br />\n",
    "In the limit as $t \\rightarrow \\pm$ we have (for  $c \\neq 0$) :\n",
    "\n",
    "$$x'(t) \\rightarrow \\frac{fa}{c}, \\,\\,\\,\\, y'(t) \\rightarrow \\frac{fb}{c}$$\n",
    "\n",
    "regardless of their starting point (that is $x_0\\,\\, and\\,\\, y_0$)\n",
    "\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"vanishing_point.png\" />\n",
    "  <center><figcaption>Fig.16(a): Vanishing Point</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"vanishing_point2.png\" />\n",
    "  <center><figcaption>Fig.16(b): Example of Vanishing Point</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Points\n",
    "\n",
    "- Each set of parallel lines (=direction) meets at a different point.\n",
    "    - The line is called the horizon for that plane\n",
    "    \n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"parallel_lines.png\" />\n",
    "  <center><figcaption>Fig.17 (a): set of parallel lines</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "#### 3-point perspective\n",
    "\n",
    "Different directions correspond to different vanishing points\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"threepoint_pers.png\" />\n",
    "  <center><figcaption>Fig.17(b): 3-point perspective</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"vanishing_point3.png\" />\n",
    "  <center><figcaption>Fig.17(c): example of vanishing point</figcaption></center>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Vision (Muller-Lyer illusion)\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"hv_pl.png\" />\n",
    "  <center><figcaption>Fig.18(a): illusion to human vision</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"hv_pl2.png\" />\n",
    "  <center><figcaption>Fig.18(b): illusion to human vision</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Quize**</font>\n",
    "\n",
    "What determines at what point in the image parallel lines intersect?\n",
    "\n",
    "\n",
    "a) The direction the lines have in the world<br />\n",
    "b) Whether the world lines are on the ground plane<br />\n",
    "c) The orientation of the camera<br />\n",
    "<font color=\"green\">d) (a) and (c)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models\n",
    "\n",
    "#### Orthograpphic Projection\n",
    "\n",
    "Special case of perspective projection\n",
    "- Distance from the COP to the image plane is infinite\n",
    "- Also called 'parallel projection':\n",
    "(x,y,z) -> (x,y)\n",
    "- The projection matrix is:\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1  \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}  \\,\\,\\, \\Rightarrow \\,\\,\\,(x,y)$$\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"orth_proj.png\" />\n",
    "  <center><figcaption>Fig.19(a): Orthograpphic Projection</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### Week Perspective\n",
    "\n",
    "- Perspective effects, but not over the scale of individual objects\n",
    "\n",
    "$$(x,y,z) \\,\\,\\,\\, \\rightarrow (\\frac{fx}{z_0},\\frac{fy}{z_0})$$\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & \\frac{1}{s}  \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1\\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\\\ \\frac{1}{s}  \\end{bmatrix}  \\,\\,\\, \\Rightarrow \\,\\,\\,(sx,sy)$$\n",
    "\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"week_per.png\" />\n",
    "  <center><figcaption>Fig.19(a): Week Perspective</figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================================\n",
    "#  3B - L1 Stereo geometry\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo: Multiple views \n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"stereo.png\" />\n",
    "  <center><figcaption>Fig.20: Stereo example</figcaption></center>\n",
    "</figure\n",
    "    \n",
    "    \n",
    "<font color=\"blue\">How images relate?</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why multiple views\n",
    "\n",
    "- Structure and depth are inherently ambiguous from single views\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"why_mul.png\" />\n",
    "  <center><figcaption>Fig.21(a): Either the man is very large or the tower is very far</figcaption></center>\n",
    "</figure>\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"why_mul2.png\" />\n",
    "  <center><figcaption>Fig.21(b): Two points projecting at the same ray causes ambigouity</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<font color=\"blue\">**Quize**</font>\n",
    "\n",
    "When you look at in image, what property indicate difference in depth, or provides hints about object shape?\n",
    "\n",
    "<font color=\"green\">motion, parallel lines, shadow, textured, focus/blur, lighting/shading, occlusion, size and scale </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do human see in 3D\n",
    "\n",
    "**Perspective effect**\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"human.png\" />\n",
    "  <center><figcaption>Fig.22(a) Perspective effect: assume houses are similar size and get smaller as distance increases</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "**Shading**\n",
    "\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"human2.png\" />\n",
    "  <center><figcaption>Fig.22(b) Shading: assuming the skin has a particular type of reflectence, the brain infere the depth from the change in the shading</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "**Texture**\n",
    "\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"human3.png\" />\n",
    "  <center><figcaption>Fig.22(c) Texture: because the texture is changing as the object tips away from you, the brain can infer the depth</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "**Focus/defocus**\n",
    "\n",
    "- Image from same point of view, different camera parameters\n",
    "- 3d shape/ depth estimates\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"human4.png\" />\n",
    "  <center><figcaption>Fig.22(d) Focus/defocus: the more you are away from the focus depth, the more out of focus you become.</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "**Motion**\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"human5.png\" />\n",
    "  <center><figcaption>Fig.22(e) Motion: from a single eye, if something move, you can infere the depth</figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating scene shape from one eye\n",
    "\n",
    "- \"Shape from X\": Shading, Texture, Focus, Motion....\n",
    "- Very popular circa 1980\n",
    "\n",
    "### But we have two eyes!\n",
    "\n",
    "**Stereo**:\n",
    "\n",
    "- The image from one eye is a little different than the image from the other eye\n",
    "- Think of shape from \"motion\" between two views\n",
    "- Infer 3D shape of scene from two (multiple) images from different viewpoints\n",
    "\n",
    "\n",
    "### Stereo Photography and Stereo Viewers\n",
    "\n",
    "Take two pictures of the same subject from slightly different viewpoints and display so that each eye sees only one of the images.\n",
    "\n",
    "<font color=\"green\">Analygph stereo: put down sort of red and blue imageery and use filters to see each image from an eye</font>\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"stereo_photo.png\" />\n",
    "  <center><figcaption>Fig.23: Example of a stereo photo. The image on the left is anaglyph stereo</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo Basic Idea\n",
    "\n",
    "The image below shows an alternation of the two images that slightly differ. Watch lecture for explanation\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"basic_idea.gif\" />\n",
    "  <center><figcaption>Fig.24: The basic idea behind stereo. Showin an alternation of the two images</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Dot Stereograms\n",
    "\n",
    "Human binuclar fusion is not based upon matching large-scale structure, or any indiviual process of the image. It is acutally based on a low-level process that directly fuses the two images.\n",
    "\n",
    "<font color=\"blue\">Watch the lesson for more explanation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Depth with Stereo\n",
    "\n",
    "\n",
    "- Assume we have two cameras, and they're both looking at some scene point, the 3D depth of a point can be recunstruected by figuring out which two points in the two cameras are that point\n",
    "\n",
    "\n",
    "- Stereo: shape from \"motion\" between two views. We'll need to consider:\n",
    "    - Image point correspondences\n",
    "    - Info on camera pose (\"calibration\")\n",
    "   \n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"stereo_math.png\" />\n",
    "  <center><figcaption>Fig.25: Depect stereo setup</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry for a simple stereo system\n",
    "\n",
    "- First, assuming parallel optical axes, known camera parameters (i.e., calibrated cameras) (Figure 26)\n",
    "- Figure is looking down on the cameras and image plane\n",
    "- Baseline $B$, focal length $f$\n",
    "- Point $p$ is distance $Z$ in camera coordinate systems\n",
    "- $Z$ is the distance all the way to center of projection (COP) not image plane. Review previous lesson\n",
    "- Point $P$ projects into left and right images.\n",
    "- Distance is positive in left image and negative in right\n",
    "\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"geometry.png\" />\n",
    "  <center><figcaption>Fig.26(a) </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"geometry2.png\" />\n",
    "  <center><figcaption>Fig.26(b) </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<font color=\"blue\">What is the expression for $Z$?</font>\n",
    "\n",
    "- Similar triangles ($p_l$, $P$, $p_r$) and ($C_l$, $P$, $C_r$)\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"geometry3.png\" />\n",
    "  <center><figcaption>Fig.26(c): first triangle </figcaption></center>\n",
    "</figure>\n",
    "<br />\n",
    "<figure>\n",
    "  <img src=\"geometry4.png\" />\n",
    "  <center><figcaption>Fig.26(d): second triangle </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "So,\n",
    "\n",
    "$$\\frac{B - x_l + x_r}{Z-f} = \\frac{B}{Z}$$\n",
    "\n",
    "$$\\Rightarrow \\,\\,\\,\\, Z = f \\frac{B}{x_l - x_r}$$\n",
    "\n",
    "\n",
    "<font color=\"blue\">What happens when disparity ($x_l - x_r$) is zero?</font>\n",
    "\n",
    "- At some point in the scene, the point is projected at the same point in the left and the right images, and thus the depth is infinit. (e.g. the moon)\n",
    "\n",
    "\n",
    "<font color=\"blue\">**Quize**</font>\n",
    "\n",
    "Which image is the left and which is right?\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"quiz_left_right.png\" />\n",
    "  <center><figcaption>Fig.26(e) quiz</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<font color=\"green\">Left is in the left image, and Right in the right one. Because we know that the person is in front of the tree, that means image on the person in the right image should be shifted to the left</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth from disparity\n",
    "\n",
    "<font color=\"blue\">More on the video</font>\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"disparity.png\" />\n",
    "  <center><figcaption>Fig.27(a): example of depth from disparity. Notice the window at the red point to the left image is shifted to the left the right image</figcaption></center>\n",
    "</figure>\n",
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"disparity_map.png\" />\n",
    "  <center><figcaption>Fig.27(b): Disparity map, for every point what is the disparity, the brighter the closer</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\frac{}{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{bmatrix} 1 & 2 & -1\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a href=\"\">Source</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<font color=\"blue\"></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "<figure>\n",
    "  <img src=\"stereo_math.png\" />\n",
    "  <center><figcaption>Fig.</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
